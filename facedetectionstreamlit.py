import streamlit as st
import numpy as np
import cv2
import onnxruntime as ort
import os
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import av
from PIL import Image

# H√†m ph√°t hi·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng Haar Cascade
def detect_faces_haar(image, cascade_classifier, scale_factor=1.1, min_neighbors=5):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = cascade_classifier.detectMultiScale(
        gray,
        scaleFactor=scale_factor,
        minNeighbors=min_neighbors,
        minSize=(30, 30)
    )
    boxes = []
    for (x, y, w, h) in faces:
        x1, y1 = x, y
        x2, y2 = x + w, y + h
        boxes.append((x1, y1, x2, y2))
    return boxes

# Danh s√°ch nh√£n
class_names = ['Dang_Cuu_Duong', 'Hoang_Manh_Tuong', 'Quan_Phan', 'Trinh_Huu_Tho']


# L·ªõp x·ª≠ l√Ω video cho streamlit-webrtc
class FaceRecognitionProcessor(VideoProcessorBase):
    def __init__(self):
        # Load Haar Cascade
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        if not os.path.exists(cascade_path):
            raise FileNotFoundError(f"Haar Cascade file not found: {cascade_path}")
        self.face_cascade = cv2.CascadeClassifier(cascade_path)
        if self.face_cascade.empty():
            raise ValueError("Failed to load Haar Cascade classifier")

        # Load VGG16 classifier
        classifier_path = r"FaceDetection\vgg_face_final.onnx"
        if not os.path.exists(classifier_path):
            raise FileNotFoundError(f"ONNX file not found: {classifier_path}")
        self.face_classifier = ort.InferenceSession(classifier_path)

    def recv(self, frame):
        # Chuy·ªÉn ƒë·ªïi frame t·ª´ av.VideoFrame sang numpy array
        img = frame.to_ndarray(format="bgr24")

        # Resize khung h√¨nh n·∫øu qu√° l·ªõn
        max_size = 1280
        if max(img.shape[:2]) > max_size:
            scale = max_size / max(img.shape[:2])
            img = cv2.resize(img, (int(img.shape[1] * scale), int(img.shape[0] * scale)))

        # Ph√°t hi·ªán khu√¥n m·∫∑t
        boxes = detect_faces_haar(img, self.face_cascade, scale_factor=1.1, min_neighbors=5)

        # Nh·∫≠n di·ªán khu√¥n m·∫∑t
        for box in boxes:
            x1, y1, x2, y2 = box
            face_crop = img[y1:y2, x1:x2]
            if face_crop.size == 0 or face_crop.shape[0] < 10 or face_crop.shape[1] < 10:
                continue

            # Chu·∫©n b·ªã ·∫£nh cho ph√¢n lo·∫°i
            face_crop = cv2.resize(face_crop, (224, 224))
            face_crop = face_crop[:, :, ::-1]  # BGR -> RGB
            face_crop = face_crop.astype(np.float32) / 255.0
            face_crop = np.expand_dims(face_crop, axis=0)

            # Ph√¢n lo·∫°i khu√¥n m·∫∑t
            inputs = {self.face_classifier.get_inputs()[0].name: face_crop}
            preds = self.face_classifier.run(None, inputs)[0]
            label_id = np.argmax(preds)
            label_name = class_names[label_id]

            # V·∫Ω bounding box v√† nh√£n
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(img, label_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        # Chuy·ªÉn ƒë·ªïi l·∫°i th√†nh av.VideoFrame
        return av.VideoFrame.from_ndarray(img, format="bgr24")


def process_image(image, processor):
    # Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh t·ª´ PIL Image sang m·∫£ng NumPy (BGR)
    img = np.array(image)[:, :, ::-1]  # RGB -> BGR

    # Resize khung h√¨nh n·∫øu qu√° l·ªõn
    max_size = 1280
    if max(img.shape[:2]) > max_size:
        scale = max_size / max(img.shape[:2])
        img = cv2.resize(img, (int(img.shape[1] * scale), int(img.shape[0] * scale)))

    # Ph√°t hi·ªán khu√¥n m·∫∑t
    boxes = detect_faces_haar(img, processor.face_cascade, scale_factor=1.1, min_neighbors=5)

    # Nh·∫≠n di·ªán khu√¥n m·∫∑t
    for box in boxes:
        x1, y1, x2, y2 = box
        face_crop = img[y1:y2, x1:x2]
        if face_crop.size == 0 or face_crop.shape[0] < 10 or face_crop.shape[1] < 10:
            continue

        # Chu·∫©n b·ªã ·∫£nh cho ph√¢n lo·∫°i
        face_crop = cv2.resize(face_crop, (224, 224))
        # Ki·ªÉm tra s·ªë k√™nh v√† chuy·ªÉn v·ªÅ 3 k√™nh (RGB) n·∫øu c·∫ßn
        if face_crop.shape[2] == 4:  # N·∫øu c√≥ k√™nh alpha (RGBA)
            face_crop = face_crop[:, :, :3]  # Ch·ªâ l·∫•y 3 k√™nh RGB
        face_crop = face_crop[:, :, ::-1]  # BGR -> RGB
        face_crop = face_crop.astype(np.float32) / 255.0
        face_crop = np.expand_dims(face_crop, axis=0)

        # In shape ƒë·ªÉ ki·ªÉm tra
        print(f"face_crop shape: {face_crop.shape}")

        # Ph√¢n lo·∫°i khu√¥n m·∫∑t
        inputs = {processor.face_classifier.get_inputs()[0].name: face_crop}
        preds = processor.face_classifier.run(None, inputs)[0]
        label_id = np.argmax(preds)
        label_name = class_names[label_id]

        # V·∫Ω bounding box v√† nh√£n
        img = np.ascontiguousarray(img)
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img, label_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    # Chuy·ªÉn l·∫°i th√†nh RGB ƒë·ªÉ hi·ªÉn th·ªã tr√™n Streamlit
    img = img[:, :, ::-1]  # BGR -> RGB
    return img

def streamlit():
    # Giao di·ªán Streamlit
    st.title("üß† Face Recognition")
    processor = FaceRecognitionProcessor()
    st.write("üë§ ·ª®ng d·ª•ng nh·∫≠n di·ªán khu√¥n m·∫∑t b·∫±ng ·∫£nh ho·∫∑c camera")

    tab1, tab2 = st.tabs(["üñºÔ∏è Nh·∫≠n di·ªán b·∫±ng ·∫£nh", "üì∑ Nh·∫≠n di·ªán b·∫±ng camera"])
    with tab1:
        st.subheader("üñºÔ∏è Nh·∫≠n di·ªán th√¥ng qua ·∫£nh")
        image_input = st.file_uploader("T·∫£i ·∫£nh mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ nh·∫≠n di·ªán", type=["jpg", "png", "tif"])

        if image_input:

            # ƒê·ªçc h√¨nh ·∫£nh t·ª´ file
            image = Image.open(image_input)
            # X·ª≠ l√Ω h√¨nh ·∫£nh
            result_img = process_image(image, processor)
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.image(result_img, "‚úÖ ·∫¢nh sau khi nh·∫≠n di·ªán", use_container_width=True)


    with tab2:
        st.subheader("üì∑ Nh·∫≠n di·ªán s·ª≠ d·ª•ng camera")
        # Kh·ªüi t·∫°o webcam stream
        webrtc_streamer(
            key="face-recognition",
            video_processor_factory=FaceRecognitionProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        st.write("‚èπÔ∏è Nh·∫•n 'Stop' ƒë·ªÉ d·ª´ng webcam ho·∫∑c ƒë√≥ng ·ª©ng d·ª•ng.")